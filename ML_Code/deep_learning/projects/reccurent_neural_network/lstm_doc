
- A quoi sert le LSTM ?

    Le LSTM prolongement du RNN sert a prédire des données sur le temps,
    mais contrairement au RNN elle peut remonter beaucoup plus loin 
    grâce a plusieurs portes, elle permet aussi de comprendre la 
    logique dans l'évolution d'une très longue séquence
    (plus d'info dans Deep Learning Concept.doc)

- Comment format les données pour l'utiliser ?

    le LSTM fonctionnent avec des données numériques scale entre 0 et 1 en input
    et utiliser le one hot encoding pour l'output, et on peut aussi utiliser
    le one hot endoding pour l'input

- Quels sont les différentes technique de LSTM ?

    1. One to One, il sagit de prédire un y à partir d'un x
    2. One to Many, il sagit de prédire une séquence y à partir d'un x
    3. Many to One, il sagit de prédire un y à partir d'une séquence de x
    4. Many to Many, il sagit de prédire une séquence de y à partir d'une séquence de x
    
    Par défaut LSTM de Keras reset les valeurs de la cell state, à chaque nouvel input (si batch_size=1)
    donc uniquement le nouvel input X passe a travers les gates.
    
    Cependant on peut modifier ce comportement avec 2 techniques :
    1. State Maintened, il sagit de garder les valeurs de la cell state durant toute la batch_size,
    et de faire la mise à jour des poids uniquement a la fin de la batch_size
    2. Stateful, c'est comme le State Maintened il sagit de préserver toute la séquence d'input 
    comme State Maintened, sauf qu'il reste indépendant par rapport à la batch_size
    Il permet contrairement à State Maintened de garder la cell state intact
    pour l'evaluation et la prédiction, mais aussi de mettre à jour les poids a chaque fin de batch_size

- Quand utiliser le Stateful ou pas, et comment ?
    - A prediction is made at the end of each sequence and sequences are independent. State
    should be reset after each sequence by setting the batch size to 1.
    - A long sequence was split into multiple subsequences (many samples each with many time
    steps). State should be reset after the network has been exposed to the entire sequence by
    making the LSTM stateful, turning off the shuffling of subsequences, and resetting the
    state after each epoch.
    - A very long sequence was split into multiple subsequences (many samples each with many
    time steps). Training efficiency is more important than the influence of long-term internal
    state and a batch size of 128 samples was used, after which network weights are updated
    and state reset.

- Quels adaptations faut-il fait pour chacune de ses techniques ?
    
    1. State Maintened : en modifiant la valeur de batch_size en l'initialisant 
    avec la taille du dataset d'input, ou moins
    2. Stateful : set le paramètre Stateful du LSTM layer à True, 
    ensuite il faut bien reset le cell state a chaque epoch pour eviter
    que la cell state de l'ancienne epoch soit appliqué à la nouvelle.
    Et finalement ne pas shuffle le dataset puisque le but ici est 
    de "comprendre" toute la séquence intrinsèque entre les input

- Quels sont les différents types de LSTM ? Voir Partie 3 de LSTM pour les examples
    - Vanilla LSTM:
        Classique LSTM qui permet d'apprendre une sequence simple
    - Stacked LSTM:
        L'idée est de stacké des LSTM pour comprendre la tendance de developpement 
        d'une série, exemple sinusoidale regressive. On peut utilise une technique
        qu'on va appeler focus_on_syntax
        qui est d'utiliser n series différentes avec la meme tendance et apprendre
        au lstm en passant chacun de ses series avec une seul epoch, batch_size x
        pour reset le state tous les x series
    - CNN LSTM : 
        L'idée est de combiner CNN qui comprendre les relations spatiales d'une image
        ou autre, avec le LSTM qui va se charger de comprendre la tendance d'évolution
        de cette espace, le CNN va quelque part préformater les données pour rendre
        la tache plus simple au LSTM, on utilise la technique focus_on_syntax
    - Encoder/Decoder LSTM :
        L'idée est de pouvoir gérer les cas où l'input et l'ouput sont des séquences
        à taille variables, mais aussi de taille variables pour entre les samples
        Pour cela on doit encoder les inputs(padding/truncate -> char to int to onehot
        et decoder les output argmax -> int to char -> invert padding/truncate
        On utilise la technique focus_on_syntax
    - Bidirectionnal LSTM :
        L'idée est de split le layer LSTM en un pour le sens normal du temps et l'autre
        pour le sens inverse, 1 2 3 4 et 4 3 2 1, de cette façon on peut obtenir de
        meilleures performances dans certains cas, ça a notamment montrer de très bon 
        résultats pour le machine translation

    
    



- Comment entrainer un LSTM ? 
    Forward and Backpropagation, seulement il y a 2 types de BP
    - Back Propagation Through Time, il sagit pour chaque
    etape de temps, de passer tous la sequence d'input dans le réseau
    (forward), puis avec les outputs de dérouler le réseau (déboucler),
    et de calculer et d'accumuler les erreurs pour chaque étapes de temps
    du réseau
    - Truncated Back Propagration Through Time:
    Comme le BPTT seulement cette fois, on choisi le nombre d'étapes de temps
    qui va servir a obtenir l'output, puis on déroule le réseau, sauf que on
    choisi le nombre d'étape de temps qui vont servir a calculer et 
    accumuler les erreurs
